# 20170405

## Aspect 入门

今天再做spark-streaming对于kafka消费一致性的问题时，需要用到aspectj。但是在学习的时候遇到了问题，目前网上aspectj的资料比较少，大部分都是spring AOP部分。

我目前急需的是：有个例子能够支持动态weave的就行了，要求不高。但是没有找到，只能阅读官网了。(>_<)

哈哈，找到一个良心文章：<a href="http://denis-zhdanov.blogspot.jp/2009/08/weaving-with-aspectj.html">http://denis-zhdanov.blogspot.jp/2009/08/weaving-with-aspectj.html</a>

还有这个： <a href="http://andrewclement.blogspot.jp/2009/02/load-time-weaving-basics.html">http://andrewclement.blogspot.jp/2009/02/load-time-weaving-basics.html</a>

有一个中文的目录（指向官网）：<a href="http://hao.jobbole.com/aspectj/">http://hao.jobbole.com/aspectj/</a>

在csdn上找到了一篇中文blog： <a href="http://blog.csdn.net/zl3450341/article/details/7673938">http://blog.csdn.net/zl3450341/article/details/7673938</a>

eclipse 官网找到了Load-Time Weaving的文章：<a href="https://eclipse.org/aspectj/doc/next/devguide/ltw.html">https://eclipse.org/aspectj/doc/next/devguide/ltw.html</a>

下面摘自stack overflow上的一个描述：<a href="http://stackoverflow.com/questions/17323500/aspect-weaving-at-runtime">http://stackoverflow.com/questions/17323500/aspect-weaving-at-runtime</a>

* aspect

An aspect is the cross-cutting functionality you are implementing. It is the aspect, or area, of your application you are modularizing. The most common (albeit simple) example of an aspect is logging. Logging is something that is required throughout an application. However, because applications tend to be broken down into layers based on functionality, reusing a logging module through inheritance

does not make sense. However, you can create a logging aspect and apply it throughout your application using AOP.

* Weaving

Weaving is the process of applying aspects to a target object to create a new, proxied object. The aspects are woven into the target object at the specified joinpoints. The weaving can take place at several points in the target class’s lifetime:

**Compile time** :Aspects are woven in when the target class is compiled. This requires a special compiler.

**Classload time** :Aspects are woven in when the target class is loaded into the JVM. This requires a special ClassLoader that enhances that target class’s bytecode before the class is introduced into the application.

**Runtime** :Aspects are woven in sometime during the execution of the application. Typically, an AOP container will dynamically generate a proxy class that will delegate to the target class while weaving in the aspects.


# 20170407

## spark streaming 和 spark core 在模型的抽象上不同

### spark streaming

* batch
* output operation 又称作 job

在spark streaming的层级上，没有retry机制， 一个job失败就是失败了，不会retry。一个batch由多个output operation组成

那一个job怎么算是失败呢？

猜测：基于spark core中的action的状态，也就是spark core中的job概念，这里会产生retry

### spark core

如果按照执行方向划分有：

* job - 按照spark core中的action划分

* stage - 按照shuffle划分不同的stage。
* task

在stage里面，再按照partition划分为不同的task。 task存在retry。这个retry比较有意思，存在几个场景：

* spark失败是

# 20170413

今天碰到一个问题，花了很久才解决，但是原理还不清楚，下面来描述一下这个现象：

有一个maven的project叫做 **data**。这个项目里有3个module，分别是 **data-a**, **data-b**, **data-c**.

结构如下：

* data
* - data-a
* - data-b
* - data-c

其中data的AVG如下：

```
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
	<groupId>com.data</groupId>
    <artifactId>data</artifactId>
    <version>2.7.2-SNAPSHOT</version>
    
    <modules>
        <module>data-a</module>
        <module>data-b</module>
        <module>data-c</module>
        
    ........
    
    <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-dependency-plugin</artifactId>
        <version>${maven-dependency-plugin.version}</version>
        <executions>
            <execution>
                <id>copy-dependencies</id>
                <phase>package</phase>
                <goals>
                    <goal>copy-dependencies</goal>
                </goals>
                <configuration>
                    <outputDirectory>target/lib</outputDirectory>
                    <overWriteReleases>
                    
     .......
                        
```

其中 data-c 依赖于 **data-a** 和 **data-b**

还有就是data-b 模块的pom里设置了

```
<build>
		<finalName>${project.artifactId}</finalName>
```

当我们对 **data-c** 打包时 

```
mvn package -pl data-c -am
```

发现：

* target/lib/data-b-2.7.2-SNAPSHOT.jar
* 在打的jar里的 META-INFO/MANIFEST.MF 中的class-path属性中是：lib/data-b.jar

这是为什么呢？ **finalName这个参数起着什么作用呢？**

# 20170416


## Java happen-before

jdk7的JSL： http://docs.oracle.com/javase/specs/jls/se7/html/jls-17.html#jls-17.4.5

java并发编程需要考虑3个因素：

* 原子性 - 指令操作和同步代码块（synchronized和lock）
* 可见性 - volatile
* 顺序性 - 

原子性的粒度是：指令级别（操作系统的指令）

### volatile

volatile体现的**可见性**。它的效果不依赖同步代码块

处理器为了提高处理速度，不直接和内存进行通讯，而是将系统内存的数据独到内部缓存后再进行操作，但操作完后不知什么时候会写到内存。 如果对声明了volatile变量进行写操作时，JVM会向处理器发送一条Lock前缀的指令，将这个变量所在缓存行的数据写会到系统内存。 但这时候其他处理器的缓存还是旧的，所以在多处理器环境下，为了保证各个处理器缓存一致，每个处理会通过嗅探在总线上传播的数据来检查 自己的缓存是否过期，当处理器发现自己缓存行对应的内存地址被修改了，就会将当前处理器的缓存行设置成无效状态，当处理器要对这个数据 进行修改操作时，会强制重新从系统内存把数据读到处理器缓存里。

volatile在java中的semantics是：对volatile修饰的变量的写对后序的读可见。这个写是赋值操作，如果对象表示修改对象的引用，而不是修改对象内部的变量。

JDK实现这个语义的方法有：
* 写的时候将cache信息同步到主存
* 读的时候将主存内容同步到cache

volatile还有其他的作用：

* 生成“屏障”（lock前缀的指令），中断指令重排
* 将之前对其他变量的写也同步到主存，这个是参考别人的文档（https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/package-summary.html）

通过synchronized和Lock也能够保证可见性，synchronized和Lock能保证同一时刻只有一个线程获取锁然后执行同步代码，并且在释放锁之前会将对变量的修改刷新到主存当中。因此可以保证可见性。这里可以看出volatile时效性更高。

推荐一个讲volatile的文章，浅显易懂：http://note.youdao.com/noteshare?id=e111564a10b7d44d137f1c9e32092442 （转自:http://www.importnew.com/24082.html）

volatile指令级别的原理：http://www.infoq.com/cn/articles/ftf-java-volatile

## Java 主进程kill 子进程

下面是JDK8中的代码

```
public abstract class Process {

    /**
     * Kills the subprocess. Whether the subprocess represented by this
     * {@code Process} object is forcibly terminated or not is
     * implementation dependent.
     */
    public abstract void destroy();
    
    /**
     * Kills the subprocess. The subprocess represented by this
     * {@code Process} object is forcibly terminated.
     *
     * <p>The default implementation of this method invokes {@link #destroy}
     * and so may not forcibly terminate the process. Concrete implementations
     * of this class are strongly encouraged to override this method with a
     * compliant implementation.  Invoking this method on {@code Process}
     * objects returned by {@link ProcessBuilder#start} and
     * {@link Runtime#exec} will forcibly terminate the process.
     *
     * <p>Note: The subprocess may not terminate immediately.
     * i.e. {@code isAlive()} may return true for a brief period
     * after {@code destroyForcibly()} is called. This method
     * may be chained to {@code waitFor()} if needed.
     *
     * @return the {@code Process} object representing the
     *         subprocess to be forcibly destroyed.
     * @since 1.8
     */
    public Process destroyForcibly() {
        destroy();
        return this;
    }
}
```

实现类

```
final class UNIXProcess extends Process {

    public void destroy() {
        destroy(false);
    }

    @Override
    public Process destroyForcibly() {
        destroy(true);
        return this;
    }
    
    private static native void destroyProcess(int pid, boolean force);
}
```

看一下这native的源码：

http://hg.openjdk.java.net/jdk8/jdk8/jdk/file/687fd7c7986d/src/solaris/native/java/lang/UNIXProcess_md.c

```
JNIEXPORT void JNICALL
Java_java_lang_UNIXProcess_destroyProcess(JNIEnv *env,
                                          jobject junk,
                                          jint pid,
                                          jboolean force)
{
    int sig = (force == JNI_TRUE) ? SIGKILL : SIGTERM;
    kill(pid, sig);
}
```


## spark streaming 如何生成batch？

InputStream表示是在Driver端生成Dstream

## DStream的transform是什么时候执行的？

## DStream的transform和action是怎么执行的？





